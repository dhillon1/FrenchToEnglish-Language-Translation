{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model for translating sentence from french language to english language"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is getting loaded from the french and english data.  After that the data is getting preprocessed to find the length and read few sentences to see how the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method for loading data\n",
    "def load_data(path):\n",
    "    #adding path to read data\n",
    "    text_data = os.path.join(path)\n",
    "    with open(text_data, \"r\") as file:\n",
    "        data = file.read()\n",
    "    return data.split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading english data and french data\n",
    "english_sentences = load_data('english_data')\n",
    "french_sentences = load_data('french_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentences: 137861\n",
      "French_sentences: 137861\n",
      "new jersey is sometimes quiet during autumn , and it is snowy in april .\n",
      "new jersey est parfois calme pendant l' automne , et il est neigeux en avril .\n"
     ]
    }
   ],
   "source": [
    "#in and out of data. Length ofeach data types\n",
    "print(\"English sentences: {}\".format(len(english_sentences)))\n",
    "print(\"French_sentences: {}\".format(len(french_sentences)))\n",
    "print(english_sentences[0])\n",
    "print(french_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn  and it is snowy in april \n",
      "new jersey est parfois calme pendant l automne  et il est neigeux en avril \n"
     ]
    }
   ],
   "source": [
    "#removing punctuation from english and french sentences\n",
    "input1 = str.maketrans('', '', string.punctuation+ '“”')\n",
    "\n",
    "#pasing punctuation from input and output sentences\n",
    "english_sentences = [w.translate(input1) for  w in english_sentences]\n",
    "french_sentences = [w.translate(input1) for  w in french_sentences]\n",
    "\n",
    "#out after removing punctuation\n",
    "print(english_sentences[0])\n",
    "print(french_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing data having more than 16 words in a sentence.\n",
    "english_data = []\n",
    "french_data = []\n",
    "for i in range(len(french_sentences)):\n",
    "    #condition on how sentences will be removed\n",
    "    if len(english_sentences[i].split())<=16 and len(french_sentences[i].split())<=16:\n",
    "        #adding sentence in new list\n",
    "        english_data.append(english_sentences[i])\n",
    "        french_data.append(french_sentences[i]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "135594\n",
      "135594\n"
     ]
    }
   ],
   "source": [
    "#length of english and french sentences after removing sentences more than 16 words \n",
    "print(len(english_data))\n",
    "print(len(french_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen out data has been reduced as few sentences were having more than 16 words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new jersey is sometimes quiet during autumn  and it is snowy in april \n",
      "new jersey est parfois calme pendant l automne  et il est neigeux en avril \n"
     ]
    }
   ],
   "source": [
    "#print of one english and french sentence after removing punctuation and caping limit to maximum of 16\n",
    "print(english_data[0])\n",
    "print(french_data[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding tokenizer instance on french data\n",
    "french_tokenizer = Tokenizer()\n",
    "#french data on tokenizer \n",
    "french_tokenizer.fit_on_texts(french_data)\n",
    "sequence_french_data = french_tokenizer.texts_to_sequences(french_data)\n",
    "    \n",
    "#adding tokenizer instance on english data\n",
    "english_tokenizer = Tokenizer()\n",
    "#english data on tokenizer \n",
    "english_tokenizer.fit_on_texts(english_data)\n",
    "sequence_english_data = english_tokenizer.texts_to_sequences(english_data)\n",
    "\n",
    "#adding padding to english and french data having words less than 16 in a sentence\n",
    "sequence_french_data = pad_sequences(sequence_french_data, maxlen=16, padding='post')\n",
    "sequence_english_data = pad_sequences(sequence_english_data, maxlen=16, padding='post')  \n",
    "\n",
    "#reshaping english data\n",
    "sequence_english_data = sequence_english_data.reshape(*sequence_english_data.shape, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "345\n",
      "200\n"
     ]
    }
   ],
   "source": [
    "#french vocab length\n",
    "french_vocab = len(french_tokenizer.word_index)+1\n",
    "print(french_vocab)\n",
    "\n",
    "#english vocab length\n",
    "english_vocab = len(english_tokenizer.word_index)+1\n",
    "print(english_vocab)\n",
    "\n",
    "#reshaping french data\n",
    "sequence_french_data = sequence_french_data.reshape((-1, sequence_english_data.shape[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are creating model using GRUs. It is a sequential model. The data is  trained in 4 epocs as the data, as model becomes over fitted after 4 epocs. We have used activation function  Relu in hidden layers  and softmax in  output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 16, 512)           176640    \n",
      "_________________________________________________________________\n",
      "gru_1 (GRU)                  (None, 16, 512)           1575936   \n",
      "_________________________________________________________________\n",
      "time_distributed_2 (TimeDist (None, 16, 1024)          525312    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 16, 1024)          0         \n",
      "_________________________________________________________________\n",
      "time_distributed_3 (TimeDist (None, 16, 200)           205000    \n",
      "=================================================================\n",
      "Total params: 2,482,888\n",
      "Trainable params: 2,482,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#creating model\n",
    "model = Sequential()\n",
    "\n",
    "#adding embedding layer\n",
    "model.add(Embedding(french_vocab,512, input_length=sequence_french_data.shape[1], \n",
    "                    input_shape=sequence_french_data.shape[1:]))\n",
    "\n",
    "#gru network\n",
    "model.add(GRU(512, return_sequences=True))    \n",
    "model.add(TimeDistributed(Dense(1024, activation='relu')))\n",
    "model.add(Dropout(0.4))\n",
    "model.add(TimeDistributed(Dense(english_vocab, activation='softmax'))) \n",
    "\n",
    "#model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model compilation for getting information\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training the model, it shows an accuracy of 84%. The accuracy can be improved by adding more data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "133/133 [==============================] - 173s 1s/step - loss: 2.9556 - accuracy: 0.4290\n",
      "Epoch 2/4\n",
      "133/133 [==============================] - 171s 1s/step - loss: 0.5651 - accuracy: 0.8168\n",
      "Epoch 3/4\n",
      "133/133 [==============================] - 174s 1s/step - loss: 0.4208 - accuracy: 0.8445\n",
      "Epoch 4/4\n",
      "133/133 [==============================] - 173s 1s/step - loss: 0.3914 - accuracy: 0.8493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x14823c070>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model training for four epocs as after that it is over fitting the model\n",
    "model.fit(sequence_french_data, sequence_english_data, batch_size=1024, epochs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is saved so that we  dont have to  train model again and again. We can use the saved model for predicting outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving model\n",
    "model.save('my_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n",
      "WARNING:absl:Found untraced functions such as gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_fn, gru_cell_layer_call_and_return_conditional_losses, gru_cell_layer_call_and_return_conditional_losses while saving (showing 5 of 5). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8t/y10bcz3j1qzbzvsz4s3wdtv80000gn/T/tmpkr92e1ci/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/8t/y10bcz3j1qzbzvsz4s3wdtv80000gn/T/tmpkr92e1ci/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9954968"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create model tflite file\n",
    "model = tf.keras.models.load_model('my_model.h5')\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "open(\"my_model.tflite\", \"wb\").write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create french tokenizer file\n",
    "import io,json\n",
    "french_tokenizer_json = french_tokenizer.to_json()\n",
    "with io.open('french_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(french_tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create english tokenizer file\n",
    "import io\n",
    "english_tokenizer_json = english_tokenizer.to_json()\n",
    "with io.open('english_tokenizer.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(english_tokenizer_json, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create english word index file\n",
    "english_word_index = []\n",
    "for i in english_tokenizer.word_index:\n",
    "    a = {i :english_tokenizer.word_index[i]}\n",
    "    english_word_index.append(a)\n",
    "\n",
    "with io.open('english_word_index.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(english_word_index, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create french word index file\n",
    "french_word_index = []\n",
    "for i in french_tokenizer.word_index:\n",
    "    a = {i :french_tokenizer.word_index[i]}\n",
    "    french_word_index.append(a)\n",
    "\n",
    "with io.open('french_word_index.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(french_word_index, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create english index word file\n",
    "english_index_word = []\n",
    "for i in english_tokenizer.index_word:\n",
    "    a = {i :english_tokenizer.index_word[i]}\n",
    "    english_index_word.append(a)\n",
    "\n",
    "with io.open('english_index_word.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(english_index_word, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create french index word file\n",
    "french_index_word = []\n",
    "for i in french_tokenizer.index_word:\n",
    "    a = {i :french_tokenizer.index_word[i]}\n",
    "    french_index_word.append(a)\n",
    "\n",
    "with io.open('french_index_word.json', 'w', encoding='utf-8') as f:\n",
    "    f.write(json.dumps(french_index_word, ensure_ascii=False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
